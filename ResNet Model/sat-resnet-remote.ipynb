{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweeping TensorFlow Script Mode Containers \n",
    "### Featuring Weights and Biases & Amazon ECR\n",
    "---\n",
    "#### Script Mode: Training and Deployment\n",
    "Script mode is a training script format for TensorFlow that lets you execute any TensorFlow training script in SageMaker with minimal modification. The [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) handles transferring your script to a SageMaker training instance. On the training instance, SageMaker's native TensorFlow support sets up training-related environment variables and executes your training script. In this example, we use a Python script to train a classification model on the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) and deploy it as an HTTPS Endpoint. \n",
    "\n",
    "#### Amazon ECR: Registering a Container\n",
    "By Dockerizing our training, we can run train our model on any AWS instance in either a single or distributed setting. \n",
    "\n",
    "\n",
    "\n",
    "#### Weights and Biases: Sweeping and Monitoring\n",
    "In addition, this notebook demonstrates how to perform real time inference with [Weights and Biases](https://wandb.com).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the environment\n",
    "\n",
    "Let's start by setting up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases\n",
    "Copy and paste your API key from https://app.wandb.ai/authorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\n",
      "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "wandb_api_key = '1d2a8e338a0ecba6e71df00638afa00b6296a83e'\n",
    "wandb_entity  = 'rosenblatt'\n",
    "wandb_project = 'satellite-model-and-orientation'\n",
    "\n",
    "!wandb login $wandb_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "The MNIST dataset has been loaded to the public S3 buckets ``sagemaker-sample-data-<REGION>`` under the prefix ``tensorflow/mnist``. There are four ``.npy`` file under this prefix:\n",
    "* ``train_data.npy``\n",
    "* ``eval_data.npy``\n",
    "* ``train_labels.npy``\n",
    "* ``eval_labels.npy``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_uri = 's3://ssa-data/dataset'\n",
    "dataset_channel = sagemaker.session.s3_input(dataset_uri, content_type='image/png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client(\"s3\")\n",
    "all_objects = s3.Bucket('ssa-data/dataset/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a script for distributed training\n",
    "\n",
    "This tutorial's training script was adapted from TensorFlow's official [CNN MNIST example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py). We have modified it to handle the ``model_dir`` parameter passed in by SageMaker. This is an S3 path which **can be used for data sharing during distributed training and checkpointing and/or model persistence**. We have also added an argument-parsing function to handle processing training-related variables.\n",
    "\n",
    "At the end of the training job we have added a step to export the trained model to the path stored in the environment variable ``SM_MODEL_DIR``, which always points to ``/opt/ml/model``. This is critical because SageMaker uploads all the model artifacts in this folder to S3 at end of training.\n",
    "\n",
    "Here is the entire script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/satellite-model-and-orientation/ResNet Model/dockerfile\n"
     ]
    }
   ],
   "source": [
    "cd dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mwandb\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcallbacks\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmatplotlib.pyplot\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mplt\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel\u001b[39;49;00m():\n",
      "    \u001b[33m\"\"\"Generate a simple model using the Keras API for Tensorflow\"\"\"\u001b[39;49;00m\n",
      "    args, unknown = _parse_args()\n",
      "    \n",
      "    config = wandb.config \u001b[37m# When Sweeping, wandb.config will be updated every session\u001b[39;49;00m\n",
      "    \n",
      "    train_ds, train_steps, val_ds, val_steps, labels = _load_dataset(args.train, config[\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m config[\u001b[33m'\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] == \u001b[33m'\u001b[39;49;00m\u001b[33mResNet\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        model = tf.keras.applications.resnet.ResNet50(include_top=\u001b[36mFalse\u001b[39;49;00m, \n",
      "                                                      weights=\u001b[33m'\u001b[39;49;00m\u001b[33mimagenet\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                                      input_shape=(\u001b[34m224\u001b[39;49;00m,\u001b[34m224\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), \n",
      "                                                      classes=\u001b[36mlen\u001b[39;49;00m(labels))\n",
      "    \u001b[34melif\u001b[39;49;00m config[\u001b[33m'\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] == \u001b[33m'\u001b[39;49;00m\u001b[33mVGG\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        model = tf.keras.applications.vgg16.VGG16(include_top=\u001b[36mFalse\u001b[39;49;00m, \n",
      "                                                      weights=\u001b[33m'\u001b[39;49;00m\u001b[33mimagenet\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                                      input_shape=(\u001b[34m224\u001b[39;49;00m,\u001b[34m224\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), \n",
      "                                                      classes=\u001b[36mlen\u001b[39;49;00m(labels))\n",
      "    \n",
      "    model = build_finetune_model(model, \n",
      "                                 [config[\u001b[33m'\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], config[\u001b[33m'\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]], \n",
      "                                 [config[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], config[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]],\n",
      "                                  config[\u001b[33m'\u001b[39;49;00m\u001b[33mactivation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \u001b[36mlen\u001b[39;49;00m(labels))\n",
      "    \n",
      "    earlystop = tf.keras.callbacks.EarlyStopping(monitor=\u001b[33m'\u001b[39;49;00m\u001b[33mloss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, mode=\u001b[33m'\u001b[39;49;00m\u001b[33mmin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, verbose=\u001b[34m1\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m config[\u001b[33m'\u001b[39;49;00m\u001b[33moptimizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] == \u001b[33m\"\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        opt = tf.keras.optimizers.Adam(learning_rate=config[\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \u001b[34melif\u001b[39;49;00m config[\u001b[33m'\u001b[39;49;00m\u001b[33moptimizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] == \u001b[33m\"\u001b[39;49;00m\u001b[33mrmsprop\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        opt = tf.keras.optimizers.RMSprop(learning_rate=config[\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \u001b[34melif\u001b[39;49;00m config[\u001b[33m'\u001b[39;49;00m\u001b[33moptimizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] == \u001b[33m\"\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        opt = tf.keras.optimizers.SGD(learning_rate=config[\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    model.compile(optimizer=opt, loss=\u001b[33m'\u001b[39;49;00m\u001b[33msparse_categorical_crossentropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    model.fit(train_ds, epochs=config[\u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], callbacks=[earlystop, \n",
      "              wandb.keras.WandbCallback()], steps_per_epoch=train_steps, \u001b[37m#callbacks=[callbacks.WandbBatchHistory()]\u001b[39;49;00m\n",
      "              validation_data=val_ds, validation_steps=val_steps, verbose=\u001b[34m0\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# maybe do some predictions\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mbuild_finetune_model\u001b[39;49;00m(base_model, dropouts, fc_layers, activation, num_classes):\n",
      "\n",
      "    x = base_model.output\n",
      "    x = tf.keras.layers.Flatten()(x)\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m fc, drop \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(fc_layers, dropouts):\n",
      "        x = tf.keras.layers.Dense(fc, activation=activation)(x) \n",
      "        x = tf.keras.layers.Dropout(drop)(x)\n",
      "\n",
      "    predictions = tf.keras.layers.Dense(num_classes, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(x)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_label\u001b[39;49;00m(file_path, labels):\n",
      "    parts  = tf.strings.split(file_path, os.path.sep)\n",
      "    \u001b[34mreturn\u001b[39;49;00m parts[-\u001b[34m2\u001b[39;49;00m] == labels\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mdecode_img\u001b[39;49;00m(img):\n",
      "    \n",
      "    img = tf.image.decode_png(img, channels=\u001b[34m3\u001b[39;49;00m)\n",
      "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
      "    \u001b[37m#####  IMAGE BLURRING GOES HERE #####\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m img\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprocess_file\u001b[39;49;00m(file_path):\n",
      "    \n",
      "    label = get_label(file_path)\n",
      "    img   = tf.io.read_file(file_path)\n",
      "    img   = decode_img(img)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m img, label\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprepare_for_training\u001b[39;49;00m(ds, size, cache=\u001b[36mTrue\u001b[39;49;00m, shuffle_buffer_size=\u001b[34m1000\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    This is a small dataset, only load it once, and keep it in memory.\u001b[39;49;00m\n",
      "\u001b[33m    use `.cache(filename)` to cache preprocessing work for datasets that don't\u001b[39;49;00m\n",
      "\u001b[33m    fit in memory.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m cache:\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(cache, \u001b[36mstr\u001b[39;49;00m):\n",
      "            ds = ds.cache(cache)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        ds = ds.cache()\n",
      "\n",
      "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
      "    ds = ds.repeat()\n",
      "    ds = ds.batch(size)\n",
      "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m ds\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_dataset\u001b[39;49;00m(base_dir):\n",
      "    \u001b[33m\"\"\"Load Satellite training data\"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# get the unique labels and number of images\u001b[39;49;00m\n",
      "    labels      = \u001b[36mlist\u001b[39;49;00m(\u001b[36mfilter\u001b[39;49;00m(\u001b[34mlambda\u001b[39;49;00m x: x != \u001b[33m'\u001b[39;49;00m\u001b[33m.DS_Store\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, os.listdir(image_dir)))\n",
      "    num_images  = \u001b[36mlen\u001b[39;49;00m(\u001b[36mlist\u001b[39;49;00m(pathlib.Path(base_dir).glob(\u001b[33m'\u001b[39;49;00m\u001b[33m*/*.png\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\n",
      "    \n",
      "    \u001b[37m# 80/20 train/test split\u001b[39;49;00m\n",
      "    num_train   = \u001b[36mint\u001b[39;49;00m(num_images * \u001b[34m0.8\u001b[39;49;00m)\n",
      "    num_val     = \u001b[36mint\u001b[39;49;00m(num_images * \u001b[34m0.2\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# calculate steps for each epoch\u001b[39;49;00m\n",
      "    train_steps = \u001b[36mint\u001b[39;49;00m(np.ceil(num_train/wand.config[\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    val_steps   = \u001b[36mint\u001b[39;49;00m(np.ceil(num_val/wand.config[\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    \n",
      "    \u001b[37m#  create the full dataset\u001b[39;49;00m\n",
      "    ds_files    = tf.data.Dataset.list_files(\u001b[36mstr\u001b[39;49;00m(pathlib.Path(base_dir)/\u001b[33m'\u001b[39;49;00m\u001b[33m*/*.png\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    \n",
      "    labeled_ds  = ds_files.map(process_file(size=wand.config[\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]), \u001b[37m# could fail here maybe\u001b[39;49;00m\n",
      "                               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
      "    \n",
      "    full_ds     = prepare_for_training(labeled_ds)\n",
      "    \n",
      "    \u001b[37m# split the dataset 80/20\u001b[39;49;00m\n",
      "    train_ds    = full_ds.take(num_train)\n",
      "    val_ds      = full_ds.skip(num_train)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m train_ds, train_steps, val_ds, val_steps, labels\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_args\u001b[39;49;00m():    \n",
      "    \u001b[33m\"\"\"Parse the arguments passed from wandb_setup.sh\"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# Set a default configuration for the model's hyperparameters\u001b[39;49;00m\n",
      "    config_defaults = {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : \u001b[34m1e-4\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : \u001b[34m10\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : \u001b[34m0.5\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mhidden\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : \u001b[34m1024\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : \u001b[34m16\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33moptimizer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : \u001b[33m\"\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mactivation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : \u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : \u001b[33m\"\u001b[39;49;00m\u001b[33mResNet\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    }\n",
      "    \n",
      "    \u001b[37m# model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sm-model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--wandb-entity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mWANDB_ENTITY\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--wandb-project\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mWANDB_PROJECT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--configuration\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=config_default)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--wandb-group\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[36mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--wandb-job-type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[36mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--wandb-sweep-id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[36mNone\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_known_args()\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[33m\"\"\"Setup Wandb according to the arguments passed from wandb_setup.sh\"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    args, unknown = _parse_args()\n",
      "    \n",
      "    \u001b[37m# Initalize Wandb using the arguments scraped by wandb_setup.sh\u001b[39;49;00m\n",
      "    wandb.init(entity   = args.wandb_entity,\n",
      "               project  = args.wandb_project, \n",
      "               group    = args.wandb_group, \n",
      "               job_type = args.wandb_job_type, \n",
      "               config   = args.configuration)\n",
      "    \n",
      "    model()\n",
      "    \n",
      "    \u001b[37m#if args.wandb_sweep_id is None:\u001b[39;49;00m\n",
      "    \u001b[37m#    model()\u001b[39;49;00m\n",
      "    \u001b[37m#else:\u001b[39;49;00m\n",
      "    \u001b[37m#    wandb.agent(args.wandb_sweep_id, model)\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow 2.0 script\n",
    "!pygmentize 'model.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Customized Docker Image\n",
    "To install specific libraries or run commands before executing your script, you need to write a custom docker file.\n",
    "\n",
    "### Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Downloads the TensorFlow library used to run the Python 3 script\u001b[39;49;00m\n",
      "\u001b[34mFROM\u001b[39;49;00m\u001b[33m tensorflow/tensorflow:2.0.0-gpu-py3\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Contains the common functionality necessary to create a container compatible with SageMaker\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install sagemaker-containers -q \n",
      "\n",
      "\u001b[37m# Wandb allows us to customize and centralize logging\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install wandb -q --upgrade\n",
      "\n",
      "\u001b[37m# Copies the training code inside the container according to the SageMaker Estimator design pattern \u001b[39;49;00m\n",
      "COPY model.py /opt/ml/code/model.py\n",
      "COPY callbacks.py /opt/ml/code/callbacks.py\n",
      "COPY wandb_setup.sh /opt/ml/code/wandb_setup.sh\n",
      "\n",
      "\u001b[37m# Set the entry point as the setup script\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m\u001b[33m SAGEMAKER_PROGRAM wandb_setup.sh\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 'Dockerfile'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wandb_setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/bin/bash\u001b[39;49;00m\n",
      " \n",
      "\u001b[37m# Argument options\u001b[39;49;00m\n",
      "\u001b[31mLONG\u001b[39;49;00m=api_key:,sweep_id:,config:,entity:,project:\n",
      "\n",
      "\u001b[37m# Parse through arguments\u001b[39;49;00m\n",
      "\u001b[34mfunction\u001b[39;49;00m args()\n",
      "{\n",
      "    \u001b[31moptions\u001b[39;49;00m=\u001b[34m$(\u001b[39;49;00mgetopt --long \u001b[31m$LONG\u001b[39;49;00m --long name: -- \u001b[33m\"\u001b[39;49;00m\u001b[31m$@\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[34m)\u001b[39;49;00m\n",
      "    [ \u001b[31m$?\u001b[39;49;00m -eq \u001b[34m0\u001b[39;49;00m ] || {\n",
      "        \u001b[36mecho\u001b[39;49;00m \u001b[33m\"wandb_setup: Incorrect option provided\"\u001b[39;49;00m\n",
      "        \u001b[36mexit\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m\n",
      "    }\n",
      "    \u001b[36meval\u001b[39;49;00m \u001b[36mset\u001b[39;49;00m -- \u001b[33m\"\u001b[39;49;00m\u001b[31m$options\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    \u001b[34mwhile\u001b[39;49;00m true; \u001b[34mdo\u001b[39;49;00m\n",
      "        \u001b[34mcase\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[31m$1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m in\n",
      "        --api_key)\n",
      "            \u001b[36mshift\u001b[39;49;00m\n",
      "            \u001b[36mexport\u001b[39;49;00m \u001b[31mWANDB_API_KEY\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[31m$1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "            ;;\n",
      "        --sweep_id)\n",
      "            shift;\n",
      "            \u001b[31mSWEEP_ID\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[31m$1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \n",
      "            ;;\n",
      "        --project)\n",
      "            \u001b[36mshift\u001b[39;49;00m\n",
      "            \u001b[36mexport\u001b[39;49;00m \u001b[31mWANDB_PROJECT\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[31m$1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "            ;;\n",
      "        --entity)\n",
      "            \u001b[36mshift\u001b[39;49;00m\n",
      "            \u001b[36mexport\u001b[39;49;00m \u001b[31mWANDB_ENTITY\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[31m$1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "            ;;\n",
      "        --config)\n",
      "            shift;\n",
      "            \u001b[31mCONFIG\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[31m$1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \n",
      "            ;;\n",
      "        --)\n",
      "            \u001b[36mbreak\u001b[39;49;00m\n",
      "            ;;\n",
      "        \u001b[34mesac\u001b[39;49;00m\n",
      "        \u001b[36mshift\u001b[39;49;00m\n",
      "    \u001b[34mdone\u001b[39;49;00m\n",
      "}\n",
      " \n",
      "args \u001b[31m$0\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[31m$@\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Colors fail on remote instance?\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Handle login\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[33m\"\u001b[39;49;00m\u001b[31m$WANDB_API_KEY\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m == \u001b[33m\"\"\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\n",
      "    \u001b[36mecho\u001b[39;49;00m -e \u001b[33m\"\\e[1;35mwandb_setup\\e[0m: \\e[31mERROR\\e[0m api_key is a required parameter. \"\u001b[39;49;00m\n",
      "    \u001b[36mecho\u001b[39;49;00m -e \u001b[33m\"\\e[1;35mwandb_setup\\e[0m: \\e[31mERROR\\e[0m Please add your api_key to your estimator's hyperparameter argument.\"\u001b[39;49;00m\n",
      "    \u001b[36mexit\u001b[39;49;00m \u001b[34m0\u001b[39;49;00m\n",
      "\u001b[34melse\u001b[39;49;00m\n",
      "    wandb login \u001b[31m$WANDB_API_KEY\u001b[39;49;00m\n",
      "\u001b[34mfi\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Handle script execution\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[33m\"\u001b[39;49;00m\u001b[31m$SWEEP_ID\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m == \u001b[33m\"\"\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\n",
      "    \u001b[36mecho\u001b[39;49;00m -e \u001b[33m\"\\e[1;35mwandb_setup\\e[0m: \\U1F3CB Performing normal training session.\"\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m [ \u001b[33m\"\u001b[39;49;00m\u001b[31m$CONFIG\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m == \u001b[33m\"\"\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\n",
      "        \u001b[36mecho\u001b[39;49;00m -e \u001b[33m\"\\e[1;35mwandb_setup\\e[0m: \\U1F3CB Using default configuration.\"\u001b[39;49;00m\n",
      "        python3 model.py\n",
      "    \u001b[34melse\u001b[39;49;00m\n",
      "        \u001b[36mecho\u001b[39;49;00m -e \u001b[33m\"\\e[1;35mwandb_setup\\e[0m: \\U1F984 Using custom configuration.\"\u001b[39;49;00m\n",
      "        python3 model.py --config \u001b[31m$CONFIG\u001b[39;49;00m\n",
      "    \u001b[34mfi\u001b[39;49;00m\n",
      "\u001b[34melse\u001b[39;49;00m\n",
      "    \u001b[36mecho\u001b[39;49;00m -e \u001b[33m\"\\e[1;35mwandb_setup\\e[0m: \\U1F9F9 Performing sweep training session.\"\u001b[39;49;00m\n",
      "    python3 model.py --wandb-sweep-id \u001b[31m$SWEEP_ID\u001b[39;49;00m\n",
      "\u001b[34mfi\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 'wandb_setup.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying our dockerfile works as expected is a two step process: first we need to build it locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  28.16kB\n",
      "Step 1/7 : FROM tensorflow/tensorflow:2.0.0-gpu-py3\n",
      "2.0.0-gpu-py3: Pulling from tensorflow/tensorflow\n",
      "\n",
      "\u001b[1B02085707: Pulling fs layer \n",
      "\u001b[1B5509d51d: Pulling fs layer \n",
      "\u001b[1B9fe70a46: Pulling fs layer \n",
      "\u001b[1Be1789921: Pulling fs layer \n",
      "\u001b[1B21d58e5d: Pulling fs layer \n",
      "\u001b[1Bfcda1e6e: Pulling fs layer \n",
      "\u001b[1Ba76e3193: Pulling fs layer \n",
      "\u001b[1B9f7e28e6: Pulling fs layer \n",
      "\u001b[1Be7aaea7e: Pulling fs layer \n",
      "\u001b[1Ba82d62e6: Pulling fs layer \n",
      "\u001b[1B420b0759: Pulling fs layer \n",
      "\u001b[1B0f532378: Pulling fs layer \n",
      "\u001b[1B8a6dc949: Pulling fs layer \n",
      "\u001b[1B1bda3d6d: Pulling fs layer \n",
      "\u001b[1B0e4900cb: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:2089bcbf3a7b9e41d7c4be3971874598d04fd0b9190aca924d634053adca41c7A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[16A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[16A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[16A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[16A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[16A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[16A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[16A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[2A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[2A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:2.0.0-gpu-py3\n",
      " ---> f7932d1761bd\n",
      "Step 2/7 : RUN pip install sagemaker-containers -q\n",
      " ---> Running in 7a528b2fc642\n",
      "\u001b[91mWARNING: You are using pip version 19.2.3, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 7a528b2fc642\n",
      " ---> 5c195e90153f\n",
      "Step 3/7 : RUN pip install wandb -q --upgrade\n",
      " ---> Running in 28edb22938cc\n",
      "\u001b[91mWARNING: You are using pip version 19.2.3, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 28edb22938cc\n",
      " ---> d311305ec625\n",
      "Step 4/7 : COPY model.py /opt/ml/code/model.py\n",
      " ---> 1949f2a39294\n",
      "Step 5/7 : COPY callbacks.py /opt/ml/code/callbacks.py\n",
      " ---> 211c91b03f81\n",
      "Step 6/7 : COPY wandb_setup.sh /opt/ml/code/wandb_setup.sh\n",
      " ---> 8d69f1ced293\n",
      "Step 7/7 : ENV SAGEMAKER_PROGRAM wandb_setup.sh\n",
      " ---> Running in 6ffb5778a311\n",
      "Removing intermediate container 6ffb5778a311\n",
      " ---> d04225354371\n",
      "Successfully built d04225354371\n",
      "Successfully tagged ssa-model:latest\n"
     ]
    }
   ],
   "source": [
    "image_name = 'ssa-model'\n",
    "!docker build -t $image_name ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a local training job using the SageMaker estimator\n",
    "\n",
    "After the docker image is built, it is automatically accessible to the local instance. To verify the job will execute as expected we create a local training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7d9d856956c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                           )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtest_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_channel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0mtrain_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"enable_sagemaker_metrics\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_sagemaker_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image, algorithm_arn, encrypt_inter_container_traffic, train_use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     def process(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         self.model_artifacts = self.container.train(\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_tmp_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         volumes = self._prepare_training_volumes(\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# If local, source directory needs to be updated to mounted /opt/ml/code path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_prepare_training_volumes\u001b[0;34m(self, data_dir, input_data_config, output_data_config, hyperparameters)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0mdata_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_source_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mvolumes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Volume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_root_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/data.py\u001b[0m in \u001b[0;36mget_data_source_instance\u001b[0;34m(data_source, sagemaker_session)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLocalFileDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"s3\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mS3DataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     raise ValueError(\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m\"data_source must be either file or s3. parsed_uri.scheme: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, bucket, prefix, sagemaker_session)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mworking_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/private{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworking_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocalFileDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/utils.py\u001b[0m in \u001b[0;36mdownload_folder\u001b[0;34m(bucket_name, prefix, target, sagemaker_session)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0m_download_files_under_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/utils.py\u001b[0m in \u001b[0;36m_download_files_under_prefix\u001b[0;34m(bucket_name, prefix, target, s3)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEEXIST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mobject_download_file\u001b[0;34m(self, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    312\u001b[0m     return self.meta.client.download_file(\n\u001b[1;32m    313\u001b[0m         \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    170\u001b[0m         return transfer.download_file(\n\u001b[1;32m    171\u001b[0m             \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             extra_args=ExtraArgs, callback=Callback)\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[1;32m    305\u001b[0m             bucket, key, filename, extra_args, subscribers)\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;31m# This is for backwards compatibility where when retries are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# exceeded we need to throw the same error from boto3 instead of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# however if a KeyboardInterrupt is raised we want want to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# out of this and propogate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# possible value integer value, which is on the scale of billions of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;31m# years...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAXINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# Once done waiting, raise an exception if present or return the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_estimator = Estimator(image_name,\n",
    "                           role,\n",
    "                           train_instance_count=1,\n",
    "                           train_instance_type='local',\n",
    "                           hyperparameters={'api_key' : wandb_api_key,\n",
    "                                            'entity'  : wandb_entity,\n",
    "                                            'project' : wandb_project}\n",
    "                          )\n",
    "\n",
    "test_estimator.fit(dataset_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering the container to Amazon Container Services (ECR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a successful training job is run on the local instance, we are now ready to push the image to Amazon Elastic Container Registry and run a training job remotely. **You may need to your role's permissions. Run the cell below and click the link.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Click this link: https://console.aws.amazon.com/iam/home?#/roles/AmazonSageMaker-ExecutionRole-20200214T172599"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "url = 'https://console.aws.amazon.com/iam/home?#/roles/' + role.split('/')[-1]\n",
    "md('Click this link: {}'.format(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Permissions\n",
    "Click on the policy with the corresponding role name, and view the JSON. If the JSON doesn't already contains the fields below, add the following to the \"Statement\" value:\n",
    "```\n",
    "{\n",
    "            \"Sid\": \"ECRPermissions\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"ecr:PutLifecyclePolicy\",\n",
    "                \"ecr:GetLifecyclePolicyPreview\",\n",
    "                \"ecr:GetDownloadUrlForLayer\",\n",
    "                \"ecr:ListTagsForResource\",\n",
    "                \"ecr:BatchDeleteImage\",\n",
    "                \"ecr:UploadLayerPart\",\n",
    "                \"ecr:ListImages\",\n",
    "                \"ecr:DeleteLifecyclePolicy\",\n",
    "                \"ecr:DeleteRepository\",\n",
    "                \"ecr:PutImage\",\n",
    "                \"ecr:UntagResource\",\n",
    "                \"ecr:SetRepositoryPolicy\",\n",
    "                \"ecr:BatchGetImage\",\n",
    "                \"ecr:CompleteLayerUpload\",\n",
    "                \"ecr:DescribeImages\",\n",
    "                \"ecr:TagResource\",\n",
    "                \"ecr:DescribeRepositories\",\n",
    "                \"ecr:StartLifecyclePolicyPreview\",\n",
    "                \"ecr:DeleteRepositoryPolicy\",\n",
    "                \"ecr:InitiateLayerUpload\",\n",
    "                \"ecr:BatchCheckLayerAvailability\",\n",
    "                \"ecr:GetLifecyclePolicy\",\n",
    "                \"ecr:GetRepositoryPolicy\",\n",
    "                \"ecr:GetAuthorizationToken\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"ECRFullAccess\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"ecr:*\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "\n",
      "COPY THIS -> \u001b[0;34m 751398683966.dkr.ecr.us-east-2.amazonaws.com/mnist-2:latest  \u001b[0m\n",
      "\n",
      "The push refers to repository [751398683966.dkr.ecr.us-east-2.amazonaws.com/mnist-2]\n",
      "f07f612567d7: Preparing\n",
      "823e33761ca8: Preparing\n",
      "6d9a87209903: Preparing\n",
      "6ffb4d81d748: Preparing\n",
      "d42640b1c1a1: Preparing\n",
      "dedee7f64028: Preparing\n",
      "529a3f87b032: Preparing\n",
      "0c2b1f7aa7ff: Preparing\n",
      "881a26fc96a8: Preparing\n",
      "b623593089c7: Preparing\n",
      "e8bc6712038a: Preparing\n",
      "9c564e8f33e8: Preparing\n",
      "75287790905f: Preparing\n",
      "0bb97e92ee41: Preparing\n",
      "718bbdc0b45f: Preparing\n",
      "4a78de7ea906: Preparing\n",
      "0bfa7a55184c: Preparing\n",
      "122be11ab4a2: Preparing\n",
      "7beb13bce073: Preparing\n",
      "f7eae43028b3: Preparing\n",
      "6cebf3abed5f: Preparing\n",
      "dedee7f64028: Waiting\n",
      "529a3f87b032: Waiting\n",
      "0c2b1f7aa7ff: Waiting\n",
      "881a26fc96a8: Waiting\n",
      "b623593089c7: Waiting\n",
      "e8bc6712038a: Waiting\n",
      "9c564e8f33e8: Waiting\n",
      "75287790905f: Waiting\n",
      "0bb97e92ee41: Waiting\n",
      "718bbdc0b45f: Waiting\n",
      "4a78de7ea906: Waiting\n",
      "0bfa7a55184c: Waiting\n",
      "122be11ab4a2: Waiting\n",
      "7beb13bce073: Waiting\n",
      "f7eae43028b3: Waiting\n",
      "6cebf3abed5f: Waiting\n",
      "823e33761ca8: Pushed\n",
      "f07f612567d7: Pushed\n",
      "6d9a87209903: Pushed\n",
      "dedee7f64028: Pushed\n",
      "529a3f87b032: Pushed\n",
      "881a26fc96a8: Pushed\n",
      "6ffb4d81d748: Pushed\n",
      "b623593089c7: Pushed\n",
      "9c564e8f33e8: Pushed\n",
      "75287790905f: Pushed\n",
      "718bbdc0b45f: Pushed\n",
      "e8bc6712038a: Pushed\n",
      "4a78de7ea906: Pushed\n",
      "122be11ab4a2: Pushed\n",
      "0bfa7a55184c: Pushed\n",
      "7beb13bce073: Pushed\n",
      "f7eae43028b3: Pushed\n",
      "d42640b1c1a1: Pushed\n",
      "6cebf3abed5f: Pushed\n",
      "0bb97e92ee41: Pushed\n",
      "0c2b1f7aa7ff: Pushed\n",
      "latest: digest: sha256:374eb6bd5830ce1e2db2a9ff0015b0301c867479e78e2a9c4d01f8a22406ca10 size: 4719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh -s \"$image_name\"\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "export fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/$1:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"$1\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"$1\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "BLUE=\"\\033[0;34m\"\n",
    "NC=\"\\033[0m\"\n",
    "echo \"\"\n",
    "echo -e \"COPY THIS -> ${BLUE}\" $fullname \" ${NC}\"\n",
    "echo \"\"\n",
    "\n",
    "docker tag $1 ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = '751398683966.dkr.ecr.us-east-2.amazonaws.com/mnist-2:latest' # <- PASTE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a remote training job using the SageMaker Estimator \n",
    "\n",
    "The `sagemaker.tensorflow.TensorFlow` estimator handles locating the script mode container, uploading your script to a S3 location and creating a SageMaker training job. Let's call out a couple important parameters here:\n",
    "\n",
    "* `py_version` is set to `'py3'` to indicate that we are using script mode since legacy mode supports only Python 2. Though Python 2 will be deprecated soon, you can use script mode with Python 2 by setting `py_version` to `'py2'` and `script_mode` to `True`.\n",
    "\n",
    "* `distributions` is used to configure the distributed training setup. It's required only if you are doing distributed training either across a cluster of instances or across multiple GPUs. Here we are using parameter servers as the distributed training schema. SageMaker training jobs run on homogeneous clusters. To make parameter server more performant in the SageMaker setup, we run a parameter server on every instance in the cluster, so there is no need to specify the number of parameter servers to launch. Script mode also supports distributed training with [Horovod](https://github.com/horovod/horovod). You can find the full documentation on how to configure `distributions` [here](https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#distributed-training). \n",
    "\n",
    "You can also initiate an estimator to train with TensorFlow 2.0 script. The only things that you will need to change are the script name and ``framework_version``. Now we verify the image by running it on the local instance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'rosenblatt-tutorial-data' # Replace with your s3 bucket name\n",
    "prefix = 'sagemaker/tf-image-mnist' # Used as part of the path in the bucket where you store data\n",
    "s3_output_location = 's3://{}/{}/{}'.format(bucket, prefix, 'mnist-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_estimator = Estimator(image_url,\n",
    "                            role,\n",
    "                            train_instance_count=1,\n",
    "                            train_instance_type='ml.m4.xlarge',\n",
    "                            output_path=s3_output_location,\n",
    "                            hyperparameters={'api_key' : wandb_api_key}\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling ``fit``\n",
    "\n",
    "To start a training job, we call `mnist_estimator.fit(training_data_uri)`.\n",
    "\n",
    "An S3 location is used here as the input. `fit` creates a default channel named `'training'`, which points to this S3 location. In the training script we can then access the training data from the location stored in `SM_CHANNEL_TRAINING`. `fit` accepts a couple other types of input as well. See the API doc [here](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.EstimatorBase.fit) for details.\n",
    "\n",
    "When training starts, the TensorFlow container executes mnist.py, passing `hyperparameters` and `model_dir` from the estimator as script arguments. Because we didn't define either in this example, no hyperparameters are passed, and `model_dir` defaults to `s3://<DEFAULT_BUCKET>/<TRAINING_JOB_NAME>`, so the script execution is as follows:\n",
    "```bash\n",
    "python mnist.py --model_dir s3://<DEFAULT_BUCKET>/<TRAINING_JOB_NAME>\n",
    "```\n",
    "When training is complete, the training job will upload the saved model for TensorFlow serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-21 20:41:23 Starting - Starting the training job...\n",
      "2020-03-21 20:41:24 Starting - Launching requested ML instances...\n",
      "2020-03-21 20:42:23 Starting - Preparing the instances for training......\n",
      "2020-03-21 20:43:22 Downloading - Downloading input data...\n",
      "2020-03-21 20:43:33 Training - Downloading the training image.........\n",
      "2020-03-21 20:45:18 Training - Training image download completed. Training in progress..\u001b[34m2020-03-21 20:45:19,275 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-21 20:45:19,275 sagemaker-containers INFO     Failed to parse hyperparameter api_key value d04fa6489b323f4a650aac3e80f17a194fe363be to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-03-21 20:45:25,533 sagemaker-containers INFO     Failed to parse hyperparameter api_key value d04fa6489b323f4a650aac3e80f17a194fe363be to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-03-21 20:45:25,536 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-21 20:45:25,548 sagemaker-containers INFO     Failed to parse hyperparameter api_key value d04fa6489b323f4a650aac3e80f17a194fe363be to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-03-21 20:45:25,550 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-21 20:45:25,563 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"api_key\": \"d04fa6489b323f4a650aac3e80f17a194fe363be\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"RemoteTest-2\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"wandb_setup.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"wandb_setup.sh\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"api_key\":\"d04fa6489b323f4a650aac3e80f17a194fe363be\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=wandb_setup.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=wandb_setup.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"api_key\":\"d04fa6489b323f4a650aac3e80f17a194fe363be\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"RemoteTest-2\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"wandb_setup.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"wandb_setup.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--api_key\",\"d04fa6489b323f4a650aac3e80f17a194fe363be\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_API_KEY=d04fa6489b323f4a650aac3e80f17a194fe363be\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/bin/sh -c ./wandb_setup.sh --api_key d04fa6489b323f4a650aac3e80f17a194fe363be\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mSuccessfully logged in to Weights & Biases!\u001b[0m\n",
      "\u001b[34m#033[1;35mwandb_setup#033[0m: 🏋 Performing normal training session.\u001b[0m\n",
      "\u001b[34m#033[1;35mwandb_setup#033[0m: 🏋 Using default configuration.\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.8.30\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in wandb/run-20200321_204529-RemoteTest-2-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Syncing run RemoteTest-2-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://app.wandb.ai/intermx/uncategorized\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://app.wandb.ai/intermx/uncategorized/runs/RemoteTest-2-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb off` to turn off syncing.\n",
      "\u001b[0m\n",
      "\u001b[34m2020-03-21 20:45:30.831428: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\u001b[0m\n",
      "\u001b[34m2020-03-21 20:45:30.831487: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (-1)\u001b[0m\n",
      "\u001b[34m2020-03-21 20:45:30.831527: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-90-200.us-east-2.compute.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2020-03-21 20:45:30.832392: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\u001b[0m\n",
      "\u001b[34m2020-03-21 20:45:30.852227: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300055000 Hz\u001b[0m\n",
      "\u001b[34m2020-03-21 20:45:30.853130: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5079420 executing computations on platform Host. Devices:\u001b[0m\n",
      "\u001b[34m2020-03-21 20:45:30.853156: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\u001b[0m\n",
      "\n",
      "2020-03-21 20:45:54 Uploading - Uploading generated training model\u001b[34mwandb: Waiting for W&B process to finish, PID 40\u001b[0m\n",
      "\u001b[34mwandb: Program ended successfully.\u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:         loss 0.007907753810286522\u001b[0m\n",
      "\u001b[34mwandb:   _timestamp 1584823547.3581598\u001b[0m\n",
      "\u001b[34mwandb:        _step 1718\u001b[0m\n",
      "\u001b[34mwandb:     _runtime 20.736751317977905\u001b[0m\n",
      "\u001b[34mwandb:     accuracy 0.9301090836524963\u001b[0m\n",
      "\u001b[34mwandb: Syncing files in wandb/run-20200321_204529-RemoteTest-2-algo-1:\u001b[0m\n",
      "\u001b[34mwandb:   code/mnist-2.py\u001b[0m\n",
      "\u001b[34mwandb: plus 7 W&B file(s) and 0 media file(s)\u001b[0m\n",
      "\u001b[34mwandb: - 0.00MB of 0.00MB uploaded#015wandb: \\ 0.00MB of 0.00MB uploaded#015wandb: | 0.00MB of 0.00MB uploaded#015wandb: / 0.00MB of 0.00MB uploaded#015wandb: - 0.00MB of 0.00MB uploaded#015wandb: \\ 0.00MB of 0.00MB uploaded#015wandb:                                                                                \u001b[0m\n",
      "\u001b[34mwandb: Synced: https://app.wandb.ai/intermx/uncategorized/runs/RemoteTest-2-algo-1\u001b[0m\n",
      "\u001b[34m2020-03-21 20:45:54,094 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-03-21 20:46:00 Completed - Training job completed\n",
      "Training seconds: 158\n",
      "Billable seconds: 158\n"
     ]
    }
   ],
   "source": [
    "mnist_estimator.fit(training_data_uri, job_name ='RemoteTest-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Sweep with Weights and Biases\n",
    "Here we will perform a hyperparameter Sweep but use a SageMaker distributed training session to speed things up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Sweep\n",
    "Don't maximize accuracy because then you can get a network with low confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: vmsf08wu\n",
      "Sweep URL: https://app.wandb.ai/intermx/sagemaker-integration/sweeps/vmsf08wu\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "  'name' : 'ChimChimCheree',\n",
    "  'descritipion' : 'A sweep is as lucky, as lucky can be',\n",
    "  'method' : 'grid',\n",
    "  'metric' : {\n",
    "      'name' : 'val_loss',\n",
    "      'goal' : 'minimize'\n",
    "  },\n",
    "  'parameters': {\n",
    "        'dropout': {\n",
    "            'values': [0.1, 0.2, 0.3, 0.4]\n",
    "        },\n",
    "        'activation': {\n",
    "            'values': ['relu', 'tanh']\n",
    "        },\n",
    "  }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, entity=wandb_entity, project=wandb_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performining the Sweep Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmpxrswv92a_algo-1-ou7pc_1 ... \n",
      "\u001b[1BAttaching to tmpxrswv92a_algo-1-ou7pc_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,377 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,377 sagemaker-containers INFO     Failed to parse hyperparameter api_key value d04fa6489b323f4a650aac3e80f17a194fe363be to Json.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Returning the value itself\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,377 sagemaker-containers INFO     Failed to parse hyperparameter sweep_id value vmsf08wu to Json.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Returning the value itself\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,378 sagemaker-containers INFO     Failed to parse hyperparameter entity value intermx to Json.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Returning the value itself\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,378 sagemaker-containers INFO     Failed to parse hyperparameter project value sagemaker-integration to Json.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Returning the value itself\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,399 sagemaker-containers INFO     Failed to parse hyperparameter api_key value d04fa6489b323f4a650aac3e80f17a194fe363be to Json.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Returning the value itself\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,399 sagemaker-containers INFO     Failed to parse hyperparameter sweep_id value vmsf08wu to Json.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Returning the value itself\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,399 sagemaker-containers INFO     Failed to parse hyperparameter entity value intermx to Json.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Returning the value itself\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,400 sagemaker-containers INFO     Failed to parse hyperparameter project value sagemaker-integration to Json.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Returning the value itself\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,402 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,414 sagemaker-containers INFO     Failed to parse hyperparameter api_key value d04fa6489b323f4a650aac3e80f17a194fe363be to Json.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Returning the value itself\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,415 sagemaker-containers INFO     Failed to parse hyperparameter sweep_id value vmsf08wu to Json.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Returning the value itself\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,415 sagemaker-containers INFO     Failed to parse hyperparameter entity value intermx to Json.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Returning the value itself\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,415 sagemaker-containers INFO     Failed to parse hyperparameter project value sagemaker-integration to Json.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Returning the value itself\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,417 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:28,430 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"current_host\": \"algo-1-ou7pc\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"framework_module\": null,\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m         \"algo-1-ou7pc\"\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m         \"api_key\": \"d04fa6489b323f4a650aac3e80f17a194fe363be\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m         \"sweep_id\": \"vmsf08wu\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m         \"entity\": \"intermx\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m         \"project\": \"sagemaker-integration\"\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m         \"training\": {\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"job_name\": \"mnist-2-2020-03-21-20-51-20-471\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"master_hostname\": \"algo-1-ou7pc\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"module_dir\": \"/opt/ml/code\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"module_name\": \"wandb_setup.sh\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"num_cpus\": 2,\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m         \"current_host\": \"algo-1-ou7pc\",\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m             \"algo-1-ou7pc\"\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m     \"user_entry_point\": \"wandb_setup.sh\"\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_HOSTS=[\"algo-1-ou7pc\"]\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_HPS={\"api_key\":\"d04fa6489b323f4a650aac3e80f17a194fe363be\",\"entity\":\"intermx\",\"project\":\"sagemaker-integration\",\"sweep_id\":\"vmsf08wu\"}\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_USER_ENTRY_POINT=wandb_setup.sh\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-ou7pc\",\"hosts\":[\"algo-1-ou7pc\"]}\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_CURRENT_HOST=algo-1-ou7pc\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_MODULE_NAME=wandb_setup.sh\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_FRAMEWORK_MODULE=\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_NUM_CPUS=2\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_MODULE_DIR=/opt/ml/code\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-ou7pc\",\"framework_module\":null,\"hosts\":[\"algo-1-ou7pc\"],\"hyperparameters\":{\"api_key\":\"d04fa6489b323f4a650aac3e80f17a194fe363be\",\"entity\":\"intermx\",\"project\":\"sagemaker-integration\",\"sweep_id\":\"vmsf08wu\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mnist-2-2020-03-21-20-51-20-471\",\"log_level\":20,\"master_hostname\":\"algo-1-ou7pc\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"wandb_setup.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-ou7pc\",\"hosts\":[\"algo-1-ou7pc\"]},\"user_entry_point\":\"wandb_setup.sh\"}\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_USER_ARGS=[\"--api_key\",\"d04fa6489b323f4a650aac3e80f17a194fe363be\",\"--entity\",\"intermx\",\"--project\",\"sagemaker-integration\",\"--sweep_id\",\"vmsf08wu\"]\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_HP_API_KEY=d04fa6489b323f4a650aac3e80f17a194fe363be\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_HP_SWEEP_ID=vmsf08wu\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_HP_ENTITY=intermx\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m SM_HP_PROJECT=sagemaker-integration\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m /bin/sh -c ./wandb_setup.sh --api_key d04fa6489b323f4a650aac3e80f17a194fe363be --entity intermx --project sagemaker-integration --sweep_id vmsf08wu\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[1;35mwandb_setup\u001b[0m: 🧹 Performing sweep training session.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.8.30\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200321_205132-mnist-2-2020-03-21-20-51-20-471-algo-1-ou7pc\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmnist-2-2020-03-21-20-51-20-471-algo-1-ou7pc\u001b[0m\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/intermx/sagemaker-integration\u001b[0m\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/intermx/sagemaker-integration/runs/mnist-2-2020-03-21-20-51-20-471-algo-1-ou7pc\u001b[0m\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:38,289 - wandb.wandb_agent - INFO - Running runs: []\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:38,528 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:38,528 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \tactivation: relu\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \tdropout: 0.1\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m wandb: Agent Started Run: tu3gvoyh\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:38.921040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:38.921131: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (-1)\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:38.921172: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (d52e7f4686ac): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:38.921700: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:38.964517: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300070000 Hz\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:38.966137: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3ab2f90 executing computations on platform Host. Devices:\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:38.966399: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:39.248427: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 172480000 exceeds 10% of system memory.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:39.414243: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 31360000 exceeds 10% of system memory.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:43,542 - wandb.wandb_agent - INFO - Running runs: ['tu3gvoyh']\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m wandb: Agent Finished Run: tu3gvoyh \n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 36\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:58,891 - wandb.wandb_agent - INFO - Cleaning up finished run: tu3gvoyh\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:59,100 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:59,101 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \tactivation: relu\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \tdropout: 0.2\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m wandb: Agent Started Run: 90p8prmo\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:59.353088: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:59.353156: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (-1)\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:59.353196: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (d52e7f4686ac): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:59.353590: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:59.361960: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300070000 Hz\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:59.362247: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3ab2f90 executing computations on platform Host. Devices:\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:59.362366: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m 2020-03-21 20:51:59.567481: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 172480000 exceeds 10% of system memory.\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m:     accuracy 0.732481062412262\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m:        _step 65\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp 1584823920.9170244\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m:     _runtime 31.288339853286743\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m:         loss 0.44380903244018555\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m: Syncing files in wandb/run-20200321_205132-mnist-2-2020-03-21-20-51-20-471-algo-1-ou7pc:\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m:   code/mnist-2.py\n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m: plus 7 W&B file(s) and 0 media file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[36malgo-1-ou7pc_1  |\u001b[0m \u001b[34m\u001b[1mwandb\u001b[0m: Synced: https://app.wandb.ai/intermx/sagemaker-integration/runs/mnist-2-2020-03-21-20-51-20-471-algo-1-ou7pc\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8f8e50631bbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                   )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmnist_estimator_sweep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mtrain_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"enable_sagemaker_metrics\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_sagemaker_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image, algorithm_arn, encrypt_inter_container_traffic, train_use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     def process(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mtraining_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LocalTrainingJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         self.model_artifacts = self.container.train(\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0m_stream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;31m# _stream_output() doesn't have the command line. We will handle the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mexit_code\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mexit_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mnist_estimator_sweep = Estimator(image_name,\n",
    "                                  role,\n",
    "                                  train_instance_count=1,\n",
    "                                  train_instance_type='local',\n",
    "                                  hyperparameters={'api_key' : wandb_api_key,\n",
    "                                                   'sweep_id': sweep_id,\n",
    "                                                   'entity'  : wandb_entity,\n",
    "                                                   'project' : wandb_project}\n",
    "                                  )\n",
    "\n",
    "mnist_estimator_sweep.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model to an endpoint\n",
    "\n",
    "The `deploy()` method creates a SageMaker model, which is then deployed to an endpoint to serve prediction requests in real time. We will use the TensorFlow Serving container for the endpoint, because we trained with script mode. This serving container runs an implementation of a web server that is compatible with SageMaker hosting protocol. The [Using your own inference code]() document explains how SageMaker runs inference containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deployed the trained TensorFlow 2.0 model to an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "predictor = mnist_estimator.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke the endpoint\n",
    "\n",
    "Let's download the training data and use that as input for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-sample-data-us-east-2/tensorflow/mnist/train_data.npy to ./train_data.npy\n",
      "download: s3://sagemaker-sample-data-us-east-2/tensorflow/mnist/train_labels.npy to ./train_labels.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "!aws --region {region} s3 cp s3://sagemaker-sample-data-{region}/tensorflow/mnist/train_data.npy train_data.npy\n",
    "!aws --region {region} s3 cp s3://sagemaker-sample-data-{region}/tensorflow/mnist/train_labels.npy train_labels.npy\n",
    "\n",
    "train_data = np.load('train_data.npy')\n",
    "train_labels = np.load('train_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formats of the input and the output data correspond directly to the request and response formats of the `Predict` method in the [TensorFlow Serving REST API](https://www.tensorflow.org/serving/api_rest). SageMaker's TensforFlow Serving endpoints can also accept additional input formats that are not part of the TensorFlow REST API, including the simplified JSON format, line-delimited JSON objects (\"jsons\" or \"jsonlines\"), and CSV data.\n",
    "\n",
    "In this example we are using a `numpy` array as input, which will be serialized into the simplified JSON format. In addtion, TensorFlow serving can also process multiple items at once as you can see in the following code. You can find the complete documentation on how to make predictions against a TensorFlow serving SageMaker endpoint [here](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_tensorflow_serving.rst#making-predictions-against-a-sagemaker-endpoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the prediction result from the TensorFlow 2.0 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction is 7, label is 7, matched: True\n",
      "prediction is 3, label is 3, matched: True\n",
      "prediction is 9, label is 4, matched: False\n",
      "prediction is 6, label is 6, matched: True\n",
      "prediction is 1, label is 1, matched: True\n",
      "prediction is 8, label is 8, matched: True\n",
      "prediction is 1, label is 1, matched: True\n",
      "prediction is 0, label is 0, matched: True\n",
      "prediction is 9, label is 9, matched: True\n",
      "prediction is 8, label is 8, matched: True\n",
      "prediction is 0, label is 0, matched: True\n",
      "prediction is 3, label is 3, matched: True\n",
      "prediction is 1, label is 1, matched: True\n",
      "prediction is 3, label is 2, matched: False\n",
      "prediction is 7, label is 7, matched: True\n",
      "prediction is 0, label is 0, matched: True\n",
      "prediction is 2, label is 2, matched: True\n",
      "prediction is 9, label is 9, matched: True\n",
      "prediction is 6, label is 6, matched: True\n",
      "prediction is 0, label is 0, matched: True\n",
      "prediction is 1, label is 1, matched: True\n",
      "prediction is 6, label is 6, matched: True\n",
      "prediction is 7, label is 7, matched: True\n",
      "prediction is 1, label is 1, matched: True\n",
      "prediction is 9, label is 9, matched: True\n",
      "prediction is 7, label is 7, matched: True\n",
      "prediction is 6, label is 6, matched: True\n",
      "prediction is 5, label is 5, matched: True\n",
      "prediction is 5, label is 5, matched: True\n",
      "prediction is 8, label is 8, matched: True\n",
      "prediction is 8, label is 8, matched: True\n",
      "prediction is 3, label is 3, matched: True\n",
      "prediction is 4, label is 4, matched: True\n",
      "prediction is 4, label is 4, matched: True\n",
      "prediction is 8, label is 8, matched: True\n",
      "prediction is 7, label is 7, matched: True\n",
      "prediction is 3, label is 3, matched: True\n",
      "prediction is 6, label is 6, matched: True\n",
      "prediction is 4, label is 4, matched: True\n",
      "prediction is 6, label is 6, matched: True\n",
      "prediction is 6, label is 6, matched: True\n",
      "prediction is 3, label is 3, matched: True\n",
      "prediction is 8, label is 8, matched: True\n",
      "prediction is 8, label is 8, matched: True\n",
      "prediction is 9, label is 9, matched: True\n",
      "prediction is 9, label is 9, matched: True\n",
      "prediction is 4, label is 4, matched: True\n",
      "prediction is 4, label is 4, matched: True\n",
      "prediction is 0, label is 0, matched: True\n",
      "prediction is 7, label is 7, matched: True\n"
     ]
    }
   ],
   "source": [
    "predictions = predictor.predict(train_data[:50])\n",
    "for i in range(0, 50):\n",
    "    prediction = np.argmax(predictions['predictions'][i])\n",
    "    label = train_labels[i]\n",
    "    print('prediction is {}, label is {}, matched: {}'.format(prediction, label, prediction == label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the endpoint\n",
    "\n",
    "Let's delete the endpoint we just created to prevent incurring any extra costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the TensorFlow 2.0 endpoint as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
